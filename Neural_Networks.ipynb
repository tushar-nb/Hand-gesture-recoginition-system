{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mz/vg384l6j0h542rj14nk8lhkh0000gn/T/ipykernel_2158/3364474064.py:5: UserWarning: viewer requires Qt\n",
      "  import skimage.viewer\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import skimage.io\n",
    "import skimage.viewer\n",
    "from skimage.io import imread\n",
    "from skimage.transform import rescale, resize\n",
    "\n",
    "from scipy import ndimage as ndi\n",
    "from skimage import feature\n",
    "\n",
    "from skimage.transform import rotate, AffineTransform\n",
    "from skimage import transform\n",
    "\n",
    "from skimage.util import random_noise\n",
    "import random\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = 0\n",
    "img_nums = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zkUK4KDkJMLp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "np.random.seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xA110drUXUFo",
    "outputId": "b7e18ce4-3f13-487b-b0e6-71434c858550"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GOrFz83aJMLp",
    "outputId": "e5fe1da0-e5d0-4352-cee4-27b8bd4a3bc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size = 90, \n",
      "Train length = 100, \n",
      "Validation length = 100\n",
      "Training Length = 663, Validation Length = 100\n"
     ]
    }
   ],
   "source": [
    "# 0: Left Swipe, 1: Right Swipe, 2: Stop, 3: Thumbs Down, 4: Thumbs Up\n",
    "train_doc = np.random.permutation(open('train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('val.csv').readlines())\n",
    "train_len = len(train_doc)\n",
    "val_len = len(val_doc)\n",
    "train_dirs = []\n",
    "val_dirs = []\n",
    "for i in range(train_doc.shape[0]):\n",
    "    train_dir = train_doc[i].split(';')[0]\n",
    "    if(i < 100):\n",
    "        val_dir = val_doc[i].split(';')[0]\n",
    "        val_dirs.append(val_dir)\n",
    "        train_dirs.append(train_dir)\n",
    "batch_size = 90                                        #experiment with the batch size\n",
    "print(f'Batch Size = {batch_size}, \\nTrain length = {len(train_dirs)}, \\nValidation length = {len(val_dirs)}')\n",
    "print(f'Training Length = {train_len}, Validation Length = {val_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size, lenth, aug_aff = 0, aug_edg = 0, aug_nois = 0):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    \n",
    "    if(img_nums == 30):\n",
    "        img_idx = range(0, 30)                                                                     #create a list of image numbers you want to use for a particular video\n",
    "    else:\n",
    "        img_idx = range(0, 30, 2)\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = lenth // batch_size                                                     # calculate the number of batches\n",
    "        num_imgs = lenth - num_batches*batch_size\n",
    "        for batch in range(num_batches):                                                          # we iterate over the number of batches\n",
    "            x = len(img_idx)\n",
    "            y = 96\n",
    "            z = 96\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3))                                           # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5))                                               # batch_labels is the one hot representation of the output\n",
    "            if(aug_aff):\n",
    "                aug_batch_data = np.zeros((batch_size,x,y,z,3))\n",
    "                aug_batch_labels = np.zeros((batch_size,5))\n",
    "            if(aug_nois):\n",
    "                nois_batch_data = np.zeros((batch_size,x,y,z,3))\n",
    "                nois_batch_labels = np.zeros((batch_size,5))\n",
    "            if(aug_edg):\n",
    "                edg_batch_data = np.zeros((batch_size,x,y,z,3))\n",
    "                edg_batch_labels = np.zeros((batch_size,5))\n",
    "\n",
    "\n",
    "\n",
    "            for folder in range(batch_size):                                                      # iterate over the batch_size\n",
    "                imgs = os.listdir('/Users/sharadchandra/Hand_Gestures/train'+'/'+ t[folder + (batch*batch_size)].split(';')[0])  # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx):                                               #  Iterate over the frames/images of a folder to read them in\n",
    "                    image = imread('/Users/sharadchandra/Hand_Gestures/train'+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "\n",
    "                    image = resize(image = image, output_shape=(y, z))\n",
    "\n",
    "                    if(aug_edg):\n",
    "                        img1 = image.astype('float64')\n",
    "                        img1[:, :, 0] += 0.2 * np.random.random(img1[:, :, 0].shape)\n",
    "                        img1[:, :, 1] += 0.2 * np.random.random(img1[:, :, 1].shape)\n",
    "                        img1[:, :, 2] += 0.2 * np.random.random(img1[:, :, 2].shape)\n",
    "                        edges10 = feature.canny(img1[:, :, 0])\n",
    "                        edges20 = feature.canny(img1[:, :, 0], sigma=3)\n",
    "                        edges11 = feature.canny(img1[:, :, 1])\n",
    "                        edges21 = feature.canny(img1[:, :, 1], sigma=3)\n",
    "                        edges12 = feature.canny(img1[:, :, 2])\n",
    "                        edges22 = feature.canny(img1[:, :, 2], sigma=3)\n",
    "                        edg_batch_data[folder, idx, :, :, 0] = edges20\n",
    "                        edg_batch_data[folder, idx, :, :, 1] = edges21\n",
    "                        edg_batch_data[folder, idx, :, :, 2] = edges22\n",
    "\n",
    "                    \n",
    "                    if(aug_nois):\n",
    "                        image_noise_aug = random_noise(image, var = 0.1)\n",
    "                        nois_batch_data[folder, idx, :, :, 0] = image_noise_aug[:, :, 0]\n",
    "                        nois_batch_data[folder, idx, :, :, 1] = image_noise_aug[:, :, 1]\n",
    "                        nois_batch_data[folder, idx, :, :, 2] = image_noise_aug[:, :, 2]\n",
    "\n",
    "\n",
    "                    image[:, :, 0] = (image [:, :, 0] - np.min(image[:, :, 0]))/(np.max(image[:, :, 0]) - np.min(image[:, :, 0]))\n",
    "                    image[:, :, 1] = (image [:, :, 1] - np.min(image[:, :, 1]))/(np.max(image[:, :, 1]) - np.min(image[:, :, 1]))\n",
    "                    image[:, :, 2] = (image [:, :, 2] - np.min(image[:, :, 2]))/(np.max(image[:, :, 2]) - np.min(image[:, :, 2]))\n",
    "\n",
    "                    if(aug_aff):\n",
    "                        shear_val = random.uniform(0.0, -0.5)\n",
    "                        tf = AffineTransform(shear = shear_val)\n",
    "                        img_aff = transform.warp(image, tf, order=1, preserve_range=True, mode='wrap')\n",
    "                        aug_batch_data[folder, idx, :, :, 0] = img_aff[:, :, 0]\n",
    "                        aug_batch_data[folder, idx, :, :, 1] = img_aff[:, :, 1]\n",
    "                        aug_batch_data[folder, idx, :, :, 2] = img_aff[:, :, 2]\n",
    "\n",
    "                    batch_data[folder,idx,:,:,0] = image[:, :, 0]       #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = image[:, :, 1]       #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = image[:, :, 2]       #normalise and feed in the image\n",
    "\n",
    "\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                if aug_aff:\n",
    "                    aug_batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "\n",
    "                if aug_nois:\n",
    "                    nois_batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                    \n",
    "                if aug_edg:\n",
    "                    edg_batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "\n",
    "            if aug_aff:\n",
    "                batch_data = np.append(batch_data, aug_batch_data, axis = 0)\n",
    "                batch_labels = np.append(batch_labels, aug_batch_labels, axis = 0) \n",
    "\n",
    "            if aug_nois:\n",
    "                batch_data = np.append(batch_data, nois_batch_data, axis = 0) \n",
    "                batch_labels = np.append(batch_labels, nois_batch_labels, axis = 0)\n",
    "                \n",
    "            if aug_edg:\n",
    "                batch_data = np.append(batch_data, edg_batch_data, axis = 0) \n",
    "                batch_labels = np.append(batch_labels, edg_batch_labels, axis = 0) \n",
    "\n",
    "\n",
    "\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "\n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        x = len(img_idx)\n",
    "        y = 96\n",
    "        z = 96\n",
    "        batch_data = np.zeros((num_imgs,x,y,z,3))                                               # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "        batch_labels = np.zeros((num_imgs,5))\n",
    "\n",
    "        if(aug_aff):\n",
    "            aug_batch_data = np.zeros((num_imgs,x,y,z,3))\n",
    "            aug_batch_labels = np.zeros((num_imgs,5))\n",
    "        if(aug_nois):\n",
    "            nois_batch_data = np.zeros((num_imgs,x,y,z,3))\n",
    "            nois_batch_labels = np.zeros((num_imgs,5))\n",
    "        if(aug_edg):\n",
    "            edg_batch_data = np.zeros((num_imgs,x,y,z,3))\n",
    "            edg_batch_labels = np.zeros((num_imgs,5))\n",
    "\n",
    "        for folder in range(num_imgs):\n",
    "          imgs = os.listdir('/Users/sharadchandra/Hand_Gestures/train'+'/'+ t[folder + (num_batches*batch_size)].split(';')[0])\n",
    "          for idx, item in enumerate(img_idx):\n",
    "            image = imread('/Users/sharadchandra/Hand_Gestures/train'+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "            #crop the images and resize them. Note that the images are of 2 different shape \n",
    "            #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "\n",
    "            image = resize(image = image, output_shape=(y, z))\n",
    "\n",
    "            if(aug_edg):\n",
    "                img1 = image.astype('float64')\n",
    "                img1[:, :, 1] += 0.2 * np.random.random(img1[:, :, 1].shape)\n",
    "                img1[:, :, 2] += 0.2 * np.random.random(img1[:, :, 2].shape)\n",
    "                edges10 = feature.canny(img1[:, :, 0])\n",
    "                edges20 = feature.canny(img1[:, :, 0], sigma=3)\n",
    "                edges11 = feature.canny(img1[:, :, 1])\n",
    "                edges21 = feature.canny(img1[:, :, 1], sigma=3)\n",
    "                edges12 = feature.canny(img1[:, :, 2])\n",
    "                edges22 = feature.canny(img1[:, :, 2], sigma=3)\n",
    "                edg_batch_data[folder, idx, :, :, 0] = edges20\n",
    "                edg_batch_data[folder, idx, :, :, 1] = edges21\n",
    "                edg_batch_data[folder, idx, :, :, 2] = edges22\n",
    "\n",
    "                    \n",
    "            if(aug_nois):\n",
    "                image_noise_aug = random_noise(image, var = 0.1)\n",
    "                nois_batch_data[folder, idx, :, :, 0] = image_noise_aug[:, :, 0]\n",
    "                nois_batch_data[folder, idx, :, :, 1] = image_noise_aug[:, :, 1]\n",
    "                nois_batch_data[folder, idx, :, :, 2] = image_noise_aug[:, :, 2]\n",
    "\n",
    "            image[:, :, 0] = (image [:, :, 0] - np.min(image[:, :, 0]))/(np.max(image[:, :, 0]) - np.min(image[:, :, 0]))\n",
    "            image[:, :, 1] = (image [:, :, 1] - np.min(image[:, :, 1]))/(np.max(image[:, :, 1]) - np.min(image[:, :, 1]))\n",
    "            image[:, :, 2] = (image [:, :, 2] - np.min(image[:, :, 2]))/(np.max(image[:, :, 2]) - np.min(image[:, :, 2]))\n",
    "\n",
    "            if(aug_aff):\n",
    "                shear_val = random.uniform(0.0, -0.5)\n",
    "                tf = AffineTransform(shear = shear_val)\n",
    "                img_aff = transform.warp(image, tf, order=1, preserve_range=True, mode='wrap')\n",
    "                aug_batch_data[folder, idx, :, :, 0] = img_aff[:, :, 0]\n",
    "                aug_batch_data[folder, idx, :, :, 1] = img_aff[:, :, 1]\n",
    "                aug_batch_data[folder, idx, :, :, 2] = img_aff[:, :, 2]\n",
    "                \n",
    "            batch_data[folder,idx,:,:,0] = image [:, :, 0]\n",
    "            batch_data[folder,idx,:,:,1] = image [:, :, 1]\n",
    "            batch_data[folder,idx,:,:,2] = image [:, :, 2]\n",
    "\n",
    "\n",
    "            batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1\n",
    "\n",
    "            if aug_aff:\n",
    "                aug_batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1\n",
    "\n",
    "            if aug_nois:\n",
    "                nois_batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1\n",
    "\n",
    "            if aug_edg:\n",
    "                edg_batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1\n",
    "\n",
    "\n",
    "                \n",
    "        if aug_aff:\n",
    "            batch_data = np.append(batch_data, aug_batch_data, axis = 0)\n",
    "            batch_labels = np.append(batch_labels, aug_batch_labels, axis = 0) \n",
    "\n",
    "        if aug_nois:\n",
    "            batch_data = np.append(batch_data, nois_batch_data, axis = 0)\n",
    "            batch_labels = np.append(batch_labels, nois_batch_labels, axis = 0) \n",
    "\n",
    "        if aug_edg:\n",
    "            batch_data = np.append(batch_data, edg_batch_data, axis = 0)\n",
    "            batch_labels = np.append(batch_labels, edg_batch_labels, axis = 0) \n",
    "\n",
    "        print(batch_data.shape, batch_labels.shape)     \n",
    "        yield batch_data, batch_labels                                                            #you yield the batch_data and the batch_labels, remember what does yield do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69MRL7veJMLr"
   },
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VCbWCgDvJMLr",
    "outputId": "62100853-6de0-47ed-8e63-2679cf19ae79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 15\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '/Users/sharadchandra/Hand_Gestures'               # '/notebooks/storage/Final_data/Collated_training/train'\n",
    "val_path = '/Users/sharadchandra/Hand_Gestures/val'                   # '/notebooks/storage/Final_data/Collated_training/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 15                                                          # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwdd2MmbJMLs"
   },
   "source": [
    "## Model\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Ah1o-bMNJMLs"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "\n",
    "#write your model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DYd3s_yNFs3r",
    "outputId": "a0b0e2f3-a2d1-4f55-8e01-a3d020cdb9e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mobilenetv2_1.00_96 (Functi  (None, 3, 3, 1280)       2257984   \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 11520)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                737344    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,995,328\n",
      "Trainable params: 737,344\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "if(cnn == 0):\n",
    "    input_shape = (96, 96, 3)\n",
    "\n",
    "    mobv2_model = applications.MobileNetV2(input_shape=input_shape, weights = 'imagenet', include_top=False)\n",
    "\n",
    "    mobv2_model.trainable = False\n",
    "#     num_layers = len(mobv2_model.layers)     \n",
    "#     print(num_layers)\n",
    "    \n",
    "#     layer_split = 4*num_layers//5\n",
    "#     print(layer_split)\n",
    "#     for layer in range(layer_split):\n",
    "#         mobv2_model.layers[layer].trainable = False\n",
    "#     for layer in range(layer_split, num_layers):\n",
    "#         mobv2_model.layers[layer].trainable = True\n",
    "        \n",
    "    cnn_model = Sequential([\n",
    "        mobv2_model,\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(64),\n",
    "#        keras.layers.BatchNormalization(),\n",
    "#        keras.layers.Dropout(0.25)\n",
    "    ])\n",
    "    cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ccMxtOtcfYVL"
   },
   "outputs": [],
   "source": [
    "# from keras.layers import Lambda\n",
    "from keras import regularizers\n",
    "\n",
    "def rnn_gru(input_dims, n_classes, cnn_model):\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(cnn_model, input_shape = input_dims))\n",
    "#    model.add(GRU(64, input_shape = input_dims, return_sequences=False, kernel_regularizer = regularizers.l2(1e-02), dropout = 0.25))\n",
    "#    model.add(GRU(64, input_shape = input_dims, return_sequences=False, dropout = 0.25))\n",
    "    model.add(GRU(64, input_shape = input_dims, return_sequences=False))\n",
    "\n",
    "    model.add(Dense(64, activation = 'relu'))\n",
    "#    model.add(BatchNormalization())\n",
    "#    model.add(Dropout(0.25))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1sTqEPmdfm_4"
   },
   "outputs": [],
   "source": [
    "if(cnn == 0):\n",
    "    if(img_nums == 30):\n",
    "        input_dimens = (30, 96, 96, 3)\n",
    "    else:\n",
    "        input_dimens = (15, 96, 96, 3)\n",
    "    model = rnn_gru(input_dims = input_dimens, n_classes = 5, cnn_model = cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFI5DkWkJMLs"
   },
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFm4sKa_o-3O"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y5OcVz_RJMLt",
    "outputId": "c4466d01-0aa5-4d14-d3cc-d1bedf316e1c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed (TimeDistr  (None, 15, 64)           2995328   \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 64)                24960     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,024,773\n",
      "Trainable params: 766,789\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "if(cnn == 0):\n",
    "    #optimiser = keras.optimizers.Adam(learning_rate = 0.01)                            #write your optimizer\n",
    "    #optimiser = 'Adam'\n",
    "    #model.compile(optimizer=keras.optimizers.Adam(lr = 0.1), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    #model.compile(optimizer=tf.train.AdamOptimizer(0.01), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    optimiser = 'Adam'\n",
    "    model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "    print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCAMysyeJMLt"
   },
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "hssZ8JAxJMLt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object generator at 0x29a917920>\n",
      "<generator object generator at 0x29a917a00>\n"
     ]
    }
   ],
   "source": [
    "if(cnn == 0):\n",
    "    aug_aff = 0\n",
    "    aug_edg = 0\n",
    "    aug_nois = 0\n",
    "    train_generator = generator('/Users/sharadchandra/Hand_Gestures/train', train_doc, batch_size, train_len, aug_aff = 1, aug_edg = 0, aug_nois = 1)\n",
    "    val_generator = generator('/Users/sharadchandra/Hand_Gestures/val', val_doc, batch_size, val_len, aug_aff = 0, aug_edg = 0, aug_nois = 0)\n",
    "    print(train_generator)\n",
    "    print(val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TcGvP3wwJMLt",
    "outputId": "1916d56b-d52b-4354-c3e1-bd79787aa444"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "if(cnn == 0):\n",
    "    model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "    if not os.path.exists(model_name):\n",
    "        os.mkdir(model_name)\n",
    "\n",
    "    filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "    LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1)     # write the REducelronplateau code here\n",
    "    callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wckZnt9SJMLu"
   },
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qb1bJ_uLJMLv",
    "outputId": "ad215d76-8c81-493a-e38b-e20b31e1ada5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps per epoch = 8, validation_steps = 2\n"
     ]
    }
   ],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "\n",
    "print(f'Steps per epoch = {steps_per_epoch}, validation_steps = {validation_steps}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txflnsPtJMLv"
   },
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "j0rtIqZoJMLv",
    "outputId": "9799ee3c-7602-4dd8-e1fe-40b5ec281a0c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mz/vg384l6j0h542rj14nk8lhkh0000gn/T/ipykernel_5143/3180245321.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mif\u001b[39;00m(cnn \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     model\u001b[39m.\u001b[39;49mfit_generator(train_generator, steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch, epochs\u001b[39m=\u001b[39;49mnum_epochs, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, \n\u001b[1;32m      3\u001b[0m                     callbacks\u001b[39m=\u001b[39;49mcallbacks_list, validation_data\u001b[39m=\u001b[39;49mval_generator, \n\u001b[1;32m      4\u001b[0m                     validation_steps\u001b[39m=\u001b[39;49mvalidation_steps, class_weight\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, workers\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, initial_epoch\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py:2507\u001b[0m, in \u001b[0;36mModel.fit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2495\u001b[0m \u001b[39m\"\"\"Fits the model on data yielded batch-by-batch by a Python generator.\u001b[39;00m\n\u001b[1;32m   2496\u001b[0m \n\u001b[1;32m   2497\u001b[0m \u001b[39mDEPRECATED:\u001b[39;00m\n\u001b[1;32m   2498\u001b[0m \u001b[39m  `Model.fit` now supports generators, so there is no longer any need to\u001b[39;00m\n\u001b[1;32m   2499\u001b[0m \u001b[39m  use this endpoint.\u001b[39;00m\n\u001b[1;32m   2500\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2501\u001b[0m warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   2502\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m`Model.fit_generator` is deprecated and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2503\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mwill be removed in a future version. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2504\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mPlease use `Model.fit`, which supports generators.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   2505\u001b[0m     stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m   2506\u001b[0m )\n\u001b[0;32m-> 2507\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m   2508\u001b[0m     generator,\n\u001b[1;32m   2509\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m   2510\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m   2511\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2512\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   2513\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_data,\n\u001b[1;32m   2514\u001b[0m     validation_steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[1;32m   2515\u001b[0m     validation_freq\u001b[39m=\u001b[39;49mvalidation_freq,\n\u001b[1;32m   2516\u001b[0m     class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[1;32m   2517\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   2518\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   2519\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   2520\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m   2521\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49minitial_epoch,\n\u001b[1;32m   2522\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/data_adapter.py:935\u001b[0m, in \u001b[0;36mGeneratorDataAdapter._peek_and_restore\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    934\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_peek_and_restore\u001b[39m(x):\n\u001b[0;32m--> 935\u001b[0m     peek \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(x)\n\u001b[1;32m    936\u001b[0m     \u001b[39mreturn\u001b[39;00m peek, itertools\u001b[39m.\u001b[39mchain([peek], x)\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if(cnn == 0):\n",
    "    model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "m2 = keras.models.load_model('/Users/sharadchandra/Hand_Gestures/model-00015-0.38187-0.84163-0.28922-0.87000.h5')#epoch_loss-train_accuracytest_losstest_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /Users/sharadchandra/Hand_Gestures ; batch size = 1\n",
      "/Users/sharadchandra/Hand_Gestures/imgs3\n",
      "SHAPE =  (96, 96, 3)\n",
      "***************** batch_data.shape = *************** (1, 15, 96, 96, 3)\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "[0.42843928933143616, 0.009935986250638962, 0.11888343840837479, 0.10312936455011368, 0.3396119773387909]\n",
      "Left Swipe\n",
      "Source path =  /Users/sharadchandra/Hand_Gestures ; batch size = 1\n",
      "/Users/sharadchandra/Hand_Gestures/imgs3\n",
      "SHAPE =  (96, 96, 3)\n",
      "***************** batch_data.shape = *************** (1, 15, 96, 96, 3)\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.15442323684692383, 0.03578585386276245, 0.44656988978385925, 0.06586601585149765, 0.2973549962043762]\n",
      "Stop\n",
      "Source path =  /Users/sharadchandra/Hand_Gestures ; batch size = 1\n",
      "/Users/sharadchandra/Hand_Gestures/imgs3\n",
      "SHAPE =  (96, 96, 3)\n",
      "***************** batch_data.shape = *************** (1, 15, 96, 96, 3)\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.20718589425086975, 0.027646053582429886, 0.2009088546037674, 0.10791924595832825, 0.456339955329895]\n",
      "Thumbs Up\n",
      "Source path =  /Users/sharadchandra/Hand_Gestures ; batch size = 1\n",
      "/Users/sharadchandra/Hand_Gestures/imgs3\n",
      "SHAPE =  (96, 96, 3)\n",
      "***************** batch_data.shape = *************** (1, 15, 96, 96, 3)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "[0.16298897564411163, 0.013168272562325, 0.06989030539989471, 0.3187409043312073, 0.43521150946617126]\n",
      "Thumbs Up\n",
      "Source path =  /Users/sharadchandra/Hand_Gestures ; batch size = 1\n",
      "/Users/sharadchandra/Hand_Gestures/imgs3\n",
      "SHAPE =  (96, 96, 3)\n",
      "***************** batch_data.shape = *************** (1, 15, 96, 96, 3)\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.04407510533928871, 0.0031628014985471964, 0.030650701373815536, 0.14437712728977203, 0.7777342796325684]\n",
      "Thumbs Up\n",
      "4\n",
      "Source path =  /Users/sharadchandra/Hand_Gestures ; batch size = 1\n",
      "/Users/sharadchandra/Hand_Gestures/imgs3\n",
      "SHAPE =  (96, 96, 3)\n",
      "***************** batch_data.shape = *************** (1, 15, 96, 96, 3)\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "[0.054675936698913574, 0.009765207767486572, 0.48839640617370605, 0.1374659538269043, 0.3096964657306671]\n",
      "Stop\n",
      "Source path =  /Users/sharadchandra/Hand_Gestures ; batch size = 1\n",
      "/Users/sharadchandra/Hand_Gestures/imgs3\n",
      "SHAPE =  (96, 96, 3)\n",
      "***************** batch_data.shape = *************** (1, 15, 96, 96, 3)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "[0.08877585828304291, 0.0062744165770709515, 0.062252145260572433, 0.29124850034713745, 0.5514490604400635]\n",
      "Thumbs Up\n",
      "4\n",
      "Source path =  /Users/sharadchandra/Hand_Gestures ; batch size = 1\n",
      "/Users/sharadchandra/Hand_Gestures/imgs3\n",
      "SHAPE =  (96, 96, 3)\n",
      "***************** batch_data.shape = *************** (1, 15, 96, 96, 3)\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.6781177520751953, 0.008557312190532684, 0.22731521725654602, 0.018843233585357666, 0.06716647744178772]\n",
      "Left Swipe\n",
      "0\n",
      "Source path =  /Users/sharadchandra/Hand_Gestures ; batch size = 1\n",
      "/Users/sharadchandra/Hand_Gestures/imgs3\n",
      "SHAPE =  (96, 96, 3)\n",
      "***************** batch_data.shape = *************** (1, 15, 96, 96, 3)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "[0.07232639938592911, 0.009869927540421486, 0.34921324253082275, 0.05065024644136429, 0.5179401636123657]\n",
      "Thumbs Up\n",
      "4\n",
      "Source path =  /Users/sharadchandra/Hand_Gestures ; batch size = 1\n",
      "/Users/sharadchandra/Hand_Gestures/imgs3\n",
      "SHAPE =  (96, 96, 3)\n",
      "***************** batch_data.shape = *************** (1, 15, 96, 96, 3)\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.39298418164253235, 0.02780568040907383, 0.26768025755882263, 0.06616177409887314, 0.2453681379556656]\n",
      "Left Swipe\n",
      "Source path =  /Users/sharadchandra/Hand_Gestures ; batch size = 1\n",
      "/Users/sharadchandra/Hand_Gestures/imgs3\n",
      "SHAPE =  (96, 96, 3)\n",
      "***************** batch_data.shape = *************** (1, 15, 96, 96, 3)\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "[0.14799462258815765, 0.01406025979667902, 0.23815296590328217, 0.12699811160564423, 0.4727939963340759]\n",
      "Thumbs Up\n",
      "Source path =  /Users/sharadchandra/Hand_Gestures ; batch size = 1\n",
      "/Users/sharadchandra/Hand_Gestures/imgs3\n",
      "SHAPE =  (96, 96, 3)\n",
      "***************** batch_data.shape = *************** (1, 15, 96, 96, 3)\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "[0.07602126151323318, 0.009392963722348213, 0.33837947249412537, 0.0675937831401825, 0.5086125731468201]\n",
      "Thumbs Up\n",
      "4\n",
      "Source path =  /Users/sharadchandra/Hand_Gestures ; batch size = 1\n",
      "/Users/sharadchandra/Hand_Gestures/imgs3\n",
      "SHAPE =  (96, 96, 3)\n",
      "***************** batch_data.shape = *************** (1, 15, 96, 96, 3)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "[0.11304188519716263, 0.0038352094125002623, 0.05027400702238083, 0.5148112177848816, 0.3180377185344696]\n",
      "Thumbs Down\n",
      "3\n",
      "Source path =  /Users/sharadchandra/Hand_Gestures ; batch size = 1\n",
      "/Users/sharadchandra/Hand_Gestures/imgs3\n",
      "SHAPE =  (96, 96, 3)\n",
      "***************** batch_data.shape = *************** (1, 15, 96, 96, 3)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "[0.028984300792217255, 0.00233351718634367, 0.08388393372297287, 0.6824836134910583, 0.20231465995311737]\n",
      "Thumbs Down\n",
      "3\n",
      "Source path =  /Users/sharadchandra/Hand_Gestures ; batch size = 1\n",
      "/Users/sharadchandra/Hand_Gestures/imgs3\n",
      "SHAPE =  (96, 96, 3)\n",
      "***************** batch_data.shape = *************** (1, 15, 96, 96, 3)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "[0.07387907058000565, 0.013547209091484547, 0.43744003772735596, 0.16978740692138672, 0.3053462505340576]\n",
      "Stop\n",
      "Source path =  /Users/sharadchandra/Hand_Gestures ; batch size = 1\n",
      "/Users/sharadchandra/Hand_Gestures/imgs3\n",
      "SHAPE =  (96, 96, 3)\n",
      "***************** batch_data.shape = *************** (1, 15, 96, 96, 3)\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.09845159947872162, 0.0057389382272958755, 0.04100622609257698, 0.4639963209629059, 0.39080697298049927]\n",
      "Thumbs Down\n",
      "Source path =  /Users/sharadchandra/Hand_Gestures ; batch size = 1\n",
      "/Users/sharadchandra/Hand_Gestures/imgs3\n",
      "SHAPE =  (96, 96, 3)\n",
      "***************** batch_data.shape = *************** (1, 15, 96, 96, 3)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "[0.07360145449638367, 0.00762183777987957, 0.17571493983268738, 0.2596767544746399, 0.4833850562572479]\n",
      "Thumbs Up\n",
      "Source path =  /Users/sharadchandra/Hand_Gestures ; batch size = 1\n",
      "/Users/sharadchandra/Hand_Gestures/imgs3\n",
      "SHAPE =  (96, 96, 3)\n",
      "***************** batch_data.shape = *************** (1, 15, 96, 96, 3)\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.3230096697807312, 0.007074163760989904, 0.1031174436211586, 0.3994542062282562, 0.16734449565410614]\n",
      "Thumbs Down\n",
      "Source path =  /Users/sharadchandra/Hand_Gestures ; batch size = 1\n",
      "/Users/sharadchandra/Hand_Gestures/imgs3\n",
      "SHAPE =  (96, 96, 3)\n",
      "***************** batch_data.shape = *************** (1, 15, 96, 96, 3)\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.28994765877723694, 0.017615005373954773, 0.4332410991191864, 0.11216706037521362, 0.14702914655208588]\n",
      "Stop\n",
      "Source path =  /Users/sharadchandra/Hand_Gestures ; batch size = 1\n",
      "/Users/sharadchandra/Hand_Gestures/imgs3\n",
      "SHAPE =  (96, 96, 3)\n",
      "***************** batch_data.shape = *************** (1, 15, 96, 96, 3)\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "[0.14007842540740967, 0.01531293150037527, 0.46576032042503357, 0.11029457300901413, 0.26855382323265076]\n",
      "Stop\n",
      "Source path =  /Users/sharadchandra/Hand_Gestures ; batch size = 1\n",
      "/Users/sharadchandra/Hand_Gestures/imgs3\n",
      "SHAPE =  (96, 96, 3)\n",
      "***************** batch_data.shape = *************** (1, 15, 96, 96, 3)\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "[0.05162125453352928, 0.006757903378456831, 0.14319577813148499, 0.5444798469543457, 0.2539452016353607]\n",
      "Thumbs Down\n",
      "3\n",
      "Source path =  /Users/sharadchandra/Hand_Gestures ; batch size = 1\n",
      "/Users/sharadchandra/Hand_Gestures/imgs3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [34], line 127\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39m#        yield l1, batch_labels\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39m#        return l1, batch_labels\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m#        return batch_data, batch_labels\u001b[39;00m\n\u001b[1;32m    126\u001b[0m p2 \u001b[39m=\u001b[39m pred_creator(train_path, train_doc, \u001b[39m1\u001b[39m, \u001b[39m15\u001b[39m)\n\u001b[0;32m--> 127\u001b[0m pred_mat1 \u001b[39m=\u001b[39m m2\u001b[39m.\u001b[39;49mpredict(p2)\n\u001b[1;32m    128\u001b[0m d \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mLeft Swipe\u001b[39m\u001b[39m'\u001b[39m , \u001b[39m'\u001b[39m\u001b[39mRight Swipe\u001b[39m\u001b[39m'\u001b[39m , \u001b[39m'\u001b[39m\u001b[39mStop\u001b[39m\u001b[39m'\u001b[39m , \u001b[39m'\u001b[39m\u001b[39mThumbs Down\u001b[39m\u001b[39m'\u001b[39m , \u001b[39m'\u001b[39m\u001b[39mThumbs Up\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    129\u001b[0m \u001b[39m# 0: Left Swipe, 1: Right Swipe, 2: Stop, 3: Thumbs Down, 4: Thumbs Up\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py:2220\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2211\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m   2212\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   2213\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUsing Model.predict with MultiWorkerMirroredStrategy \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2214\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mor TPUStrategy and AutoShardPolicy.FILE might lead to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2217\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m   2218\u001b[0m         )\n\u001b[0;32m-> 2220\u001b[0m data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mget_data_handler(\n\u001b[1;32m   2221\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m   2222\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   2223\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps,\n\u001b[1;32m   2224\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m   2225\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m   2226\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   2227\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   2228\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   2229\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   2230\u001b[0m     steps_per_execution\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution,\n\u001b[1;32m   2231\u001b[0m )\n\u001b[1;32m   2233\u001b[0m \u001b[39m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[1;32m   2234\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(callbacks, callbacks_module\u001b[39m.\u001b[39mCallbackList):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/data_adapter.py:1582\u001b[0m, in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1580\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(kwargs[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39m_cluster_coordinator\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1581\u001b[0m     \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 1582\u001b[0m \u001b[39mreturn\u001b[39;00m DataHandler(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/data_adapter.py:1262\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1259\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution \u001b[39m=\u001b[39m steps_per_execution\n\u001b[1;32m   1261\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m-> 1262\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[1;32m   1263\u001b[0m     x,\n\u001b[1;32m   1264\u001b[0m     y,\n\u001b[1;32m   1265\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   1266\u001b[0m     steps\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m   1267\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs \u001b[39m-\u001b[39;49m initial_epoch,\n\u001b[1;32m   1268\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1269\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m   1270\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   1271\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   1272\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   1273\u001b[0m     distribution_strategy\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdistribute\u001b[39m.\u001b[39;49mget_strategy(),\n\u001b[1;32m   1274\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1275\u001b[0m )\n\u001b[1;32m   1277\u001b[0m strategy \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy()\n\u001b[1;32m   1279\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/data_adapter.py:869\u001b[0m, in \u001b[0;36mGeneratorDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(x, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    867\u001b[0m \u001b[39m# Since we have to know the dtype of the python generator when we build\u001b[39;00m\n\u001b[1;32m    868\u001b[0m \u001b[39m# the dataset, we have to look at a batch to infer the structure.\u001b[39;00m\n\u001b[0;32m--> 869\u001b[0m peek, x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_peek_and_restore(x)\n\u001b[1;32m    870\u001b[0m peek \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_standardize_batch(peek)\n\u001b[1;32m    871\u001b[0m peek \u001b[39m=\u001b[39m _process_tensorlike(peek)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/data_adapter.py:935\u001b[0m, in \u001b[0;36mGeneratorDataAdapter._peek_and_restore\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    934\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_peek_and_restore\u001b[39m(x):\n\u001b[0;32m--> 935\u001b[0m     peek \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(x)\n\u001b[1;32m    936\u001b[0m     \u001b[39mreturn\u001b[39;00m peek, itertools\u001b[39m.\u001b[39mchain([peek], x)\n",
      "Cell \u001b[0;32mIn [34], line 107\u001b[0m, in \u001b[0;36mpred_creator\u001b[0;34m(source_path, folder_list, batch_size, lenth)\u001b[0m\n\u001b[1;32m    103\u001b[0m image[:, :, \u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m (image [:, :, \u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39mmin(image[:, :, \u001b[39m1\u001b[39m]))\u001b[39m/\u001b[39m(np\u001b[39m.\u001b[39mmax(image[:, :, \u001b[39m1\u001b[39m]) \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39mmin(image[:, :, \u001b[39m1\u001b[39m]))\n\u001b[1;32m    104\u001b[0m image[:, :, \u001b[39m2\u001b[39m] \u001b[39m=\u001b[39m (image [:, :, \u001b[39m2\u001b[39m] \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39mmin(image[:, :, \u001b[39m2\u001b[39m]))\u001b[39m/\u001b[39m(np\u001b[39m.\u001b[39mmax(image[:, :, \u001b[39m2\u001b[39m]) \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39mmin(image[:, :, \u001b[39m2\u001b[39m]))\n\u001b[0;32m--> 107\u001b[0m image \u001b[39m=\u001b[39m resize(image \u001b[39m=\u001b[39;49m image, output_shape\u001b[39m=\u001b[39;49m(y, z))\n\u001b[1;32m    109\u001b[0m batch_data[folder,idx,:,:,\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m image[:, :, \u001b[39m0\u001b[39m]       \u001b[39m#normalise and feed in the image\u001b[39;00m\n\u001b[1;32m    110\u001b[0m batch_data[folder,idx,:,:,\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m image[:, :, \u001b[39m1\u001b[39m]       \u001b[39m#normalise and feed in the image\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/skimage/transform/_warps.py:181\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(image, output_shape, order, mode, cval, clip, preserve_range, anti_aliasing, anti_aliasing_sigma)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[39melif\u001b[39;00m np\u001b[39m.\u001b[39many((anti_aliasing_sigma \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m) \u001b[39m&\u001b[39m (factors \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[1;32m    179\u001b[0m             warn(\u001b[39m\"\u001b[39m\u001b[39mAnti-aliasing standard deviation greater than zero but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    180\u001b[0m                  \u001b[39m\"\u001b[39m\u001b[39mnot down-sampling along all axes\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 181\u001b[0m     image \u001b[39m=\u001b[39m ndi\u001b[39m.\u001b[39;49mgaussian_filter(image, anti_aliasing_sigma,\n\u001b[1;32m    182\u001b[0m                                 cval\u001b[39m=\u001b[39;49mcval, mode\u001b[39m=\u001b[39;49mndi_mode)\n\u001b[1;32m    184\u001b[0m \u001b[39mif\u001b[39;00m NumpyVersion(scipy\u001b[39m.\u001b[39m__version__) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m1.6.0\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    185\u001b[0m     \u001b[39m# The grid_mode kwarg was introduced in SciPy 1.6.0\u001b[39;00m\n\u001b[1;32m    186\u001b[0m     zoom_factors \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m \u001b[39m/\u001b[39m f \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m factors]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scipy/ndimage/_filters.py:342\u001b[0m, in \u001b[0;36mgaussian_filter\u001b[0;34m(input, sigma, order, output, mode, cval, truncate)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(axes) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    341\u001b[0m     \u001b[39mfor\u001b[39;00m axis, sigma, order, mode \u001b[39min\u001b[39;00m axes:\n\u001b[0;32m--> 342\u001b[0m         gaussian_filter1d(\u001b[39minput\u001b[39;49m, sigma, axis, order, output,\n\u001b[1;32m    343\u001b[0m                           mode, cval, truncate)\n\u001b[1;32m    344\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m output\n\u001b[1;32m    345\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scipy/ndimage/_filters.py:261\u001b[0m, in \u001b[0;36mgaussian_filter1d\u001b[0;34m(input, sigma, axis, order, output, mode, cval, truncate)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[39m# Since we are calling correlate, not convolve, revert the kernel\u001b[39;00m\n\u001b[1;32m    260\u001b[0m weights \u001b[39m=\u001b[39m _gaussian_kernel1d(sigma, order, lw)[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m--> 261\u001b[0m \u001b[39mreturn\u001b[39;00m correlate1d(\u001b[39minput\u001b[39;49m, weights, axis, output, mode, cval, \u001b[39m0\u001b[39;49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scipy/ndimage/_filters.py:133\u001b[0m, in \u001b[0;36mcorrelate1d\u001b[0;34m(input, weights, axis, output, mode, cval, origin)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mInvalid origin; origin must satisfy \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    130\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39m-(len(weights) // 2) <= origin <= \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    131\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39m(len(weights)-1) // 2\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    132\u001b[0m mode \u001b[39m=\u001b[39m _ni_support\u001b[39m.\u001b[39m_extend_mode_to_code(mode)\n\u001b[0;32m--> 133\u001b[0m _nd_image\u001b[39m.\u001b[39;49mcorrelate1d(\u001b[39minput\u001b[39;49m, weights, axis, output, mode, cval,\n\u001b[1;32m    134\u001b[0m                       origin)\n\u001b[1;32m    135\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "source_path = \"/Users/sharadchandra/Hand_Gestures/train\"\n",
    "mnp = 0\n",
    "flag_2 = 0\n",
    "exit = 0\n",
    "\n",
    "\n",
    "while(True):\n",
    "\n",
    "    directory = 'imgs3'\n",
    "    location = \"/Users/sharadchandra/Hand_Gestures/train\"\n",
    "    # Parent Directory path\n",
    "    parent_dir = location\n",
    "    dir = \"imgs3\"\n",
    "    # Path\n",
    "    path2 = os.path.join(parent_dir, directory)\n",
    "    path = os.path.join(location, dir)\n",
    "    # Create the directory\n",
    "    # 'GeeksForGeeks' in\n",
    "    # '/home / User / Documents'\n",
    "    os.mkdir(path)\n",
    "\n",
    "\n",
    "    import cv2\n",
    "    from time import *\n",
    "\n",
    "    # Opens the inbuilt camera of laptop to capture video.\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    i = 0\n",
    "    if(mnp==0):\n",
    "        sleep(2)\n",
    "    mnp+=1\n",
    "\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "        sleep(0.01)\n",
    "        # This condition prevents from infinite looping\n",
    "        # incase video ends.\n",
    "        if ret == False:\n",
    "            break\n",
    "        #cv2.imshow('frame', frame)\n",
    "        # Save Frame by Frame into disk using imwrite method\n",
    "        #cv2.imwrite('Frame'+str(i)+'.jpg', frame)\n",
    "        path = '/Users/sharadchandra/Hand_Gestures/train/imgs3'\n",
    "        cv2.imwrite(os.path.join(path ,  str(i) + '.png'), frame)\n",
    "        i += 1\n",
    "        if (i == 45):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def pred_creator(source_path, folder_list, batch_size, lenth ):\n",
    "        print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "\n",
    "        l = []\n",
    "        i = 0\n",
    "\n",
    "        if(img_nums == 30):\n",
    "            img_idx = range(0, 30)                                                                     #create a list of image numbers you want to use for a particular video\n",
    "        else:\n",
    "            img_idx = range(0, 30, 2)\n",
    "\n",
    "        t = np.random.permutation(folder_list)\n",
    "        #t = folder_list\n",
    "        num_batches = lenth // batch_size                                                     # calculate the number of batches\n",
    "        num_batches = 1\n",
    "        num_imgs = lenth - num_batches*batch_size\n",
    "        for batch in range(num_batches):                                                          # we iterate over the number of batches\n",
    "            x = len(img_idx)\n",
    "            y = 96\n",
    "            z = 96\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3))                                           # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "    #           batch_labels = np.zeros((batch_size,5))                                               # batch_labels is the one hot representation of the output\n",
    "            batch_labels = np.zeros((30, 5))                                               # batch_labels is the one hot representation of the output\n",
    "\n",
    "\n",
    "    # WIN_20180925_17_44_26_Pro_Right_Swipe_new;Right_Swipe_new;1\n",
    "\n",
    "\n",
    "            for folder in range(batch_size):                                                      # iterate over the batch_size\n",
    "    #            imgs = os.listdir(source_path+'/'+ 'WIN_20180925_17_08_43_Pro_Left_Swipe_new')\n",
    "                #imgs = os.listdir(source_path+'/'+ 'WIN_20180925_17_44_26_Pro_Right_Swipe_new')\n",
    "                print(source_path + '/' + 'imgs3')\n",
    "                imgs = os.listdir('/Users/sharadchandra/Hand_Gestures/train'+'/'+ 'imgs3')\n",
    "                #imgs = sorted(imgs)\n",
    "                #print(imgs)                                                                       # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx):                                               #  Iterate over the frames/images of a folder to read them in\n",
    "    #                    print(\"This Img Path is: \")\n",
    "    #                    print(source_path+'/'+ 'WIN_20180925_17_08_43_Pro_Left_Swipe_new'+'/'+imgs[item])\n",
    "    #                image = imread(source_path+'/'+ 'WIN_20180925_17_44_26_Pro_Right_Swipe_new'+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = imread('/Users/sharadchandra/Hand_Gestures/train'+'/'+ 'imgs3'+'/'+imgs[item]).astype(np.float32)\n",
    "\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "\n",
    "                    image[:, :, 0] = (image [:, :, 0] - np.min(image[:, :, 0]))/(np.max(image[:, :, 0]) - np.min(image[:, :, 0]))\n",
    "                    image[:, :, 1] = (image [:, :, 1] - np.min(image[:, :, 1]))/(np.max(image[:, :, 1]) - np.min(image[:, :, 1]))\n",
    "                    image[:, :, 2] = (image [:, :, 2] - np.min(image[:, :, 2]))/(np.max(image[:, :, 2]) - np.min(image[:, :, 2]))\n",
    "\n",
    "\n",
    "                    image = resize(image = image, output_shape=(y, z))\n",
    "\n",
    "                    batch_data[folder,idx,:,:,0] = image[:, :, 0]       #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = image[:, :, 1]       #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = image[:, :, 2]       #normalise and feed in the image\n",
    "\n",
    "    #                l.append(image)\n",
    "    #               batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                batch_labels[folder, 0]= 1\n",
    "        print(\"SHAPE = \" , image.shape)\n",
    "    #        l.append(batch_labels)\n",
    "            #return l\n",
    "    #        l1 = np.array(l)\n",
    "        print(\"***************** batch_data.shape = ***************\", batch_data.shape)\n",
    "        yield batch_data, batch_labels\n",
    "    #        yield l1, batch_labels\n",
    "    #        return l1, batch_labels\n",
    "    #        return batch_data, batch_labels\n",
    "\n",
    "    p2 = pred_creator(train_path, train_doc, 1, 15)\n",
    "    pred_mat1 = m2.predict(p2)\n",
    "    d = ['Left Swipe' , 'Right Swipe' , 'Stop' , 'Thumbs Down' , 'Thumbs Up']\n",
    "    # 0: Left Swipe, 1: Right Swipe, 2: Stop, 3: Thumbs Down, 4: Thumbs Up\n",
    "\n",
    "    l_pred = pred_mat1.tolist()\n",
    "    l_pred = l_pred[0]\n",
    "    v = 0\n",
    "    flag_2 = 0\n",
    "    print(l_pred)\n",
    "    for i in range(len(l_pred)):\n",
    "        if l_pred[i] == max(l_pred):\n",
    "            print (d[i])\n",
    "            v = i\n",
    "            break\n",
    "    flag = 0\n",
    "    for i in l_pred:\n",
    "        if i>0.5:\n",
    "            flag = 1\n",
    "    if(flag):\n",
    "        from pynput.keyboard import Key, Controller\n",
    "        from time import *\n",
    "\n",
    "        keyboard = Controller()\n",
    "\n",
    "        def spacebar():\n",
    "            keyboard.press(Key.space)\n",
    "            keyboard.release(Key.space)\n",
    "\n",
    "        def right():\n",
    "            keyboard.press(Key.right)\n",
    "            keyboard.release(Key.right)\n",
    "\n",
    "        def left():\n",
    "            keyboard.press(Key.left)\n",
    "            keyboard.release(Key.left)\n",
    "\n",
    "        def up():\n",
    "            keyboard.press(Key.up)\n",
    "            keyboard.release(Key.up)\n",
    "\n",
    "        def down():\n",
    "            keyboard.press(Key.down)\n",
    "            keyboard.release(Key.down)\n",
    "        def escp():\n",
    "            keyboard.press(Key.esc)\n",
    "            keyboard.press(Key.esc)\n",
    "        def enter():\n",
    "            keyboard.press(Key.f5)\n",
    "            keyboard.press(Key.f5)\n",
    "\n",
    "        x = v\n",
    "        print(x)\n",
    "        if x == 0:\n",
    "            sleep(0.5)\n",
    "            left()\n",
    "        if x == 1:\n",
    "            sleep(0.5)\n",
    "            right()\n",
    "        if x == 2:\n",
    "            sleep(5)\n",
    "            #left()\n",
    "        if x == 3:\n",
    "            sleep(0.5)\n",
    "            flag_2 = 1\n",
    "            break\n",
    "        if x == 4:\n",
    "            sleep(0.5)\n",
    "            if (exit == 0):\n",
    "                escp()\n",
    "                exit += 1\n",
    "            else:\n",
    "                enter()\n",
    "                exit-=1\n",
    "                \n",
    "              \n",
    "\n",
    "    # Python program to demonstrate\n",
    "    # shutil.rmtree()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # location\n",
    "    location = \"/Users/sharadchandra/Hand_Gestures/train\"\n",
    "\n",
    "    # directory\n",
    "    dir = \"imgs3\"\n",
    "\n",
    "    # path\n",
    "    path = os.path.join(location, dir)\n",
    "\n",
    "    # removing directory\n",
    "    shutil.rmtree(path)\n",
    "    if(flag_2):\n",
    "        break\n",
    "\n",
    "if(flag_2 == 1):\n",
    "    dir = \"imgs3\"\n",
    "\n",
    "    # path\n",
    "    path = os.path.join(location, dir)\n",
    "\n",
    "    # removing directory\n",
    "    shutil.rmtree(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.5.5.64-cp36-abi3-win_amd64.whl (35.4 MB)\n",
      "Requirement already satisfied: numpy>=1.14.5; python_version >= \"3.7\" in c:\\users\\vrishank\\anaconda3\\lib\\site-packages (from opencv-python) (1.19.5)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.5.5.64\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "! pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pynput\n",
      "  Downloading pynput-1.7.6-py2.py3-none-any.whl (89 kB)\n",
      "Requirement already satisfied: six in c:\\users\\vrishank\\anaconda3\\lib\\site-packages (from pynput) (1.15.0)\n",
      "Installing collected packages: pynput\n",
      "Successfully installed pynput-1.7.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install pynput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/sharadchandra/Hand_Gestures/train/imgs3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [23], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(location, \u001b[39mdir\u001b[39m)\n\u001b[1;32m      9\u001b[0m     \u001b[39m# removing directory\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m shutil\u001b[39m.\u001b[39;49mrmtree(path)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/shutil.py:714\u001b[0m, in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    712\u001b[0m     orig_st \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mlstat(path)\n\u001b[1;32m    713\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m--> 714\u001b[0m     onerror(os\u001b[39m.\u001b[39;49mlstat, path, sys\u001b[39m.\u001b[39;49mexc_info())\n\u001b[1;32m    715\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/shutil.py:712\u001b[0m, in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[39m# Note: To guard against symlink races, we use the standard\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \u001b[39m# lstat()/open()/fstat() trick.\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 712\u001b[0m     orig_st \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mlstat(path)\n\u001b[1;32m    713\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    714\u001b[0m     onerror(os\u001b[39m.\u001b[39mlstat, path, sys\u001b[39m.\u001b[39mexc_info())\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/sharadchandra/Hand_Gestures/train/imgs3'"
     ]
    }
   ],
   "source": [
    "location = \"/Users/sharadchandra/Hand_Gestures/train\"\n",
    "\n",
    "    # directory\n",
    "dir = \"imgs3\"\n",
    "\n",
    "    # path\n",
    "path = os.path.join(location, dir)\n",
    "\n",
    "    # removing directory\n",
    "shutil.rmtree(path)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Neural_Nets_Project_Starter_Code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
